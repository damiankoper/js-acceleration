\section{Benchmarking}\label{sec:benchmarking}

We tested the performance of mentioned acceleration methods in different environments. Implementation status and reason if not implemented is shown in table~\ref{tab:implemented}. Tested JS environments and their versions are described in table~\ref{tab:versions}.


As an algorithm to benchmark, we chose a simplified standard variant of Hough transform(SHT) \cite{mukhopadhyay2015survey}. Choosing a single algorithm over the whole benchmark suite gives us granular control over implementation, building process, and adaptation for each acceleration method. Hough transform, in the standard variant, is used to detect lines in binary images. It maps points to values in an accumulator space, called parameter space. Unlike the original \cite{hough1962method}, modern version maps points to curves $(x, y)$ using polar coordinates $(\theta, \rho)$ according to (\ref{eq:polar_hough}) \cite{duda1972use}.
Peaks in the parameter space can be mapped back to line candidates.
\begin{equation}
    \label{eq:polar_hough}
    f(x,y) = \rho(\theta) = x\cos{\theta}+y\sin{\theta}
\end{equation}


The resolution of the accumulator determines the precision of line detection, the size of the computational problem, and the required memory. The computational complexity of the sequential algorithm equals $O(wh)$ where $w$ and $h$ are dimensions of an input image. It could be also expressed as $O(S_{\theta} S_\rho)$ with constant input dimensions where $S_{\theta}$ and $S_\rho$ denotes angular and pixel sampling respectively. We benchmark each method for various problem sizes keeping everything constant but $S_\theta$. We implemented a version simplified from commonly seen ones. Our implementation defines the anchor point of polar coordinates in the upper left corner of the image instead of its center. It also bases the voting process on a simple threshold instead of analyzing the image space~\cite{palmer1997optimizing}.

\input{tables/implemented}
We believe that the algorithm being a representation of CPU-intensive task is sufficient for performing valuable benchmarks across different environments since it requires many iterations to fill the accumulator and enforces intensified memory usage for input data and the accumulator. Relying heavily on $\sin$ and $\cos$ functions, also allows us to test their performance. Because of that, we implemented two variants of each algorithm - \textit{non-LUT} and \textit{LUT}. The first one uses standard $\sin$ and $\cos$ functions and the second one caches their results in a lookup table. As shown in Figure \ref{fig:sht_example}, we also use the image after threshold operation instead of commonly used edge-detection. It requires more pixels to be processed thus increases problem size.


\input{tables/versions}
Each benchmark lasts $5s$ minimum and $30s$ maximum or $50$ runs. We want to rely on the most likely execution scenarios which are influenced by JS engine optimizations, resulting in a shorter execution time. To resolve this cold start problem we use the coefficient of variance metric ($c_v$). We start the actual benchmark after the window of $5$ executions where $c_v \le 1\permil$.

\input{figures/sht_example}
Benchmarks were performed on a platform equipped with Intel\textsuperscript{\tiny\textregistered} Core\textsuperscript{\tiny\texttrademark} i7-12700KF CPU and Nvidia 970 GTX GPU using Ubuntu 20.04.1. The CPU had frequency scaling turned off and due to its hybrid architecture only 4 P-Cores were enabled for a benchmark using \texttt{taskset} utility.
