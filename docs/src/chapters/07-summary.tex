\chapter{Podsumowanie}

Czy język JavaScript, jego ekosystem i~środowiska są przystosowane do wykonywania intensywnych obliczeń numerycznych? Czy asynchroniczny model wykonania wspiera, czy też przeszkadza w~wykonywaniu obliczeń? Czy środowiska zapewniają odpowiednie mechanizmy, aby wygodnie ładować dane, przyspieszać obliczenia oraz wyświetlać wyniki? Która z~metod akceleracji jest najbardziej wydajna oraz jak wygląda ich kompatybilność wśród środowisk. Czy istnieje możliwość bezkompromisowego utworzenia biblioteki kompatybilnej ze wszystkimi środowiskami? Na te i~inne pytania postarano się uzyskać odpowiedź w~tej pracy. Nie skorzystano w~tym celu z~syntetycznych testów wydajności środowisk języka JavaScript i~dostępnych metod akceleracji obliczeń oraz równoległej do nich analizy ich kompatybilności i~metod budowania bibliotek. Zamiast tego zastosowano podejście całościowe. Zaimplementowano algorytmy przetwarzania obrazów bazujące na transformacji Hough'a. Jest ona wykorzystywana w~algorytmach detekcji kształtów parametrycznych i~nieparametrycznych. W~zależności od kształtu i~wydajności powstały różne jej warianty. W~ramach tej pracy zaimplementowano wariant \textit{Standard} (Standard Hough Transform, SHT) i~\textit{Circle} (Circle Hough Transform, CHT) z~wykorzystaniem metody analizy gradientu do detekcji środków okręgów. Jako format eksportowanych bibliotek przyjęto moduły ES6 (ESModules).

Dla każdej z~analizowanych metod akceleracji przeprowadzono pomiary czasu wykonania we wszystkich udostępniających ją środowiskach. Do badanych środowisk należały przeglądarki internetowe Google Chrome i~Mozilla Firefox oraz środowiska serwerowe NodeJS i~Deno. Wszystkie z~nich korzystają z~V8 jako silnika języka JavaScript, co razem z~implementacją dla środowiska z~silnikiem SpiderMonkey - przeglądarki Mozilla Firefox, umożliwiło ich wzajemne porównanie. 

Jako definicję metody akceleracji przyjęto wszelkie działania, które potencjalnie mogą przyspieszyć wykonania algorytmu. Zaimplementowano algorytmy w~wariancie sekwencyjnym. Algorytm SHT dodatkowo został zaimplementowany z~wykorzystaniem stablicowanych wartości funkcji trygonometrycznych, co w~dużym stopniu zwiększyło wydajność. Do testów pozostałych metod akceleracji wykorzystano zatem trzy implementacje - dwie algorytmu SHT i~jedną CHT.

Podstawową metodą akceleracji była poprawa wydajności wykonania sekwencyjnego. Kolejną z~nich stanowiło wykorzystanie natywnych rozszerzeń środowiska NodeJS, których bazę stanowił kod C++. Następną z~nich było WebAssembly, którego bazę również stanowił kod w~języku C++ kompilowany do niskopoziomowego języka kompatybilnego z~wszystkimi środowiskami. Oprócz standardowej kompilacji, kod C++ skompilowano do kodu \textit{asm.js} oraz uruchomiono procesy automatycznej wektoryzacji wykonania podczas kompilacji do WebAssembly, aby wykorzystać wektorowe rejestry i~instrukcje procesora (SIMD). Przeprowadzono również kompilację po manualnej wektoryzacji kodu. Pierwszą z~metod wykorzystujących współbieżne wykonanie jest implementacja Worker'ów, gdzie zrównoleglone zostały iteracje najbardziej obciążonych pętli w~algorytmach. Ostatnią z~analizowanych metod było wykorzystanie potoku graficznego WebGL do przeprowadzenia masowo równoległych obliczeń.

Dla zaimplementowanych metod akceleracji podjęto próbę stworzenia biblioteki kompatybilnej ze wszystkimi środowiskami, co nie zawsze było możliwe. Zadbano jednak, aby każda z~bibliotek implementowała ten sam interfejs. Było to zapewnione przez transpilator języka TypeScript, w~którym to zaimplementowano wszystkie elementy bibliotek. Podczas transpilacji plików źródłowych do języka JavaScript, korzystał on z~tych samych symboli definiujących interfejs eksportowanych przez bibliotekę obiektów i~funkcji. Biblioteka zbudowana dla wariantu sekwencyjnego jest kompatybilna ze wszystkimi środowiskami, ponieważ opiera się tylko i~wyłącznie na standardowych mechanizmach języka JavaScript. Metoda wykorzystująca natywne rozszerzenia została zbudowana tylko dla środowiska NodeJS, które jako jedyne z~badanych udostępnia ją w~tej formie. Środowisko Deno również pozwala na zbudowanie i~wykonanie natywnych rozszerzeń, jednak bazą dla nich jest język Rust. Nie zostały one zaimplementowane, ponieważ implementacja algorytmów w~tym języku wykracza poza zakres tej pracy, która skupia się na bazowym kodzie w~języku C++. Dla tej metody, niezbędne było utworzenie warstwy powiązania, która zarządzała kopiowaniem danych oraz wywoływaniem właściwych funkcji. Narzędzie \textit{node-gyp} wraz z~Node-API niestety nie udostępniają automatycznych mechanizmów konwersji obiektów i~tablic z~języka JavaScript na ich odpowiedniki w~języku C++. Rodziło to konieczność ręcznego pobierania wartości do struktur na podstawie obiektu kontekstu wykonania. Kolejna zaimplementowana metoda wykorzystuje WebAssembly. Również tam niezbędne było utworzenie warstwy powiązania, jednak narzędzie \textit{Emscripten} udostępnia mechanizm automatycznej konwersji obiektów po zdefiniowaniu ich struktury, co zdecydowanie upraszcza proces implementacji. Jako, że proces inicjalizacji modułu WebAssembly musi odbyć się asynchronicznie, oraz nie zaimplementowano w~kodzie C++ mechanizmów łączenia zagnieżdżonych struktur, aby zapewnić domyślne wartości obiektom opcji, w~implementacji konieczne okazało się opakowanie modułu WebAssembly w~obiekt, który zajmuje się jego inicjalizacją oraz dostarcza domyślne dane opcji. Samo wywołanie funkcji implementowanych algorytmów odbywa się synchronicznie. Kompilacja z~użyciem narzędzia \textit{Emscripten}, aby uzyskać zakładany format modułu, wymaga użycia dodatkowych flag, co ma miejsce również w~przypadku wariacji formatów wyjściowych (\textit{asm.js}) i~używanych instrukcji (SIMD). Również i~w tym wypadku wszystkie środowiska, na których przeprowadzone zostały badania tej metody akceleracji, były kompatybilne z~tym samym kodem źródłowym.

Kompatybilności jednego kodu źródłowego ze wszystkimi środowiskami nie udało się uzyskać w~przypadku implementacji Worker'ów. Pomimo użycia biblioteki Comlink, która nałożyła warstwę abstrakcji ułatwiając komunikację głównego wątku z~Worker'ami, konieczne było utworzenie dwóch wersji kodu osobno dla środowisk przeglądarek internetowych i~osobno dla NodeJS. Środowisko Deno również wymagało utworzenia osobnej implementacji, ale w~odróżnieniu od pozostałych środowisk, gdzie biblioteka została zbudowana z~plików źródłowych, jest ono w~stanie wykonać pliki w~języku TypeScript. Konieczność budowania wielu wersji biblioteki wymusza niekompatybilny pomiędzy środowiskami przeglądarek, Deno i~NodeJS interfejs obsługi Worker'ów. Dlatego też, aby zapewnić możliwość współdzielenia kodu samego algorytmu dla tej metody akceleracji, niezbędne jest użycie adapterów oraz zbudowanie osobnych wersji biblioteki dla każdego ze środowisk.

Kolejną metodą niekompatybilną ze wszystkimi badanymi środowiskami jest wykorzystanie GPGPU poprzez użycie WebGL API, czyli potoku graficznego do obliczeń ogólnego przeznaczenia. Jest ona możliwa do wdrożenia natywnie jedynie w~przeglądarkach internetowych. W~NodeJS istnieje konieczność z~zainstalowania zewnętrznej implementacji WebGL, a~środowisko Deno nie wspiera jej wcale. Metoda ta, w~przeciwieństwie do użycia GPGPU w~przystosowanych do tego rozwiązaniach, nie umożliwia interakcji z~pamięcią współdzieloną pomiędzy wątkami, a~wynikiem obliczeń jest tylko kolor piksela. Jako liczba może być on jednak dowolnie interpretowany. Metoda ta, pomimo swoich wad i~wykorzystania potoku niezgodnie z~jego przeznaczeniem, może przynieść zdecydowaną poprawę wydajności algorytmów. Muszą one jednak być podatne na masowe zrównoleglenie oraz nie korzystać z~pamięci współdzielonej. Przykładem najprostszego z~nich może być mnożenie macierzy, czy operacja splotu. Dołączona biblioteka \textit{gpu.js}, służąca do obsługi interakcji z~potokiem graficznym, transpiluje kod JavaScript na kod programu \textit{Fragment Shader} potoku graficznego. Proces budowania musi uwzględniać mechanizmy minifikacji kodu, ponieważ mogą one interferować z~ową transpilacją i~produkować kod i~jego konstrukcje niezrozumiałe przez bibliotekę \textit{gpu.js}.

\input{tables/envs.tex}

W tabeli \ref{tab:envs} przedstawiono podsumowanie czasów wszystkich pomiarów wydajności dla wartości próbkowanie $S_\theta=1$ dla wariantu SHT algorytmu i~współczynnikiem maksymalnego promienia $n=10$ dla CHT. Wykonanie dla algorytmu CHT z~metodą WebGL nie zostało uwzględnione, ponieważ w~implementacji wystąpiły błędy, co opisano w~sekcji \ref{sec:cht-gpu}. W~ogólnym podsumowaniu najbardziej wydajnym środowiskiem okazało się Google Chrome. Kolejnymi środowiskami pod względem wydajności są NodeJS i~Deno, których wydajność jest porównywalna. Najwolniejszym ze środowisk jest przeglądarka Mozilla Firefox. Osiągnęła ona wyniki porównywalne z~Google Chrome jedynie w~przypadku akceleracji z~użyciem WebGL, gdzie ciężar obliczeń oddelegowany jest do GPU, a~przeglądarka odpowiedzialna jest za przekazanie danych i~skompilowanie programów shader'ów.

Najlepszą z~metod, których bazę stanowił kod w~C++, jest użycie natywnych rozszerzeń w~środowisku NodeJS. WebAssembly, również przynosi poprawę wydajności we wszystkich środowiskach w~porównaniu do wykonania sekwencyjnego. Wykorzystanie automatycznej wektoryzacji kodu, aby wykorzystać instrukcje i~rejestry wektorowe nie przyniosło rezultatów w~żadnym środowisku w~porównaniu do zwykłego WebAssembly. Manualna wektoryzacja, która wymagała modyfikacji kodu C++ i~użycia specjalnych instrukcji operujących na tych rejestrach, przyniosła poprawę wydajności dla algorytmów SHT, którego dane wejściowe wymuszały większą intensywność obliczeń w~modyfikowanych pętlach. Dla algorytmów CHT manualna wektoryzacja nie przyniosła żadnych rezultatów, bądź nawet spowolniła wykonanie. Pozwala to wysnuć wniosek, że manualna wektoryzacja dołożyła czas związany z~jej obsługą, który zrekompensowany został poprzez intensywne jej wykorzystanie, co miało miejsce dla testów algorytmów SHT i~nie dla CHT. Metoda wykorzystujące \textit{asm.js} jako jedyna nie przyniosła poprawy wydajności. Spowodowane to być może sposobem budowania biblioteki z~kodem \textit{asm.js}, który uniemożliwił prawidłową jego interpretacje przez przeglądarki, ale również to, że \textit{asm.js} jest natywnie wspierany - kompilowany Ahead-of-Time jedynie w~przeglądarce Mozilla Firefox.

Największe przyspieszenie umożliwiło wykorzystanie Worker'ów oraz WebGL z~oczywistą przewagą tego drugiego. Wyniki te nie dziwią, ponieważ naturalnym jest osiągnięcie lepszych wyników wykonując obliczenia równolegle. Przewaga WebGL jest o~tyle specyficzna, że implementowany algorytm musi dać się masowo zrównoleglić bez wpływu na opisane wcześniej wady tej metody, podczas gdy nie trzeba się o~to martwić w~przypadku Worker'ów. 

Badania te pozwalają stwierdzić, że środowiska języka JavaScript są gotowe, aby zapewnić wysoką wydajność obliczeń numerycznych. Wszystkie zaimplementowane metody akceleracji poza \textit{asm.js} zmniejszyły lub nie zmieniły czasu wykonania w~ramach środowiska. Dalsze prace nad testowaniem wydajności mogą sprawdzać działanie hybrydowych metod akceleracji, czego przykładem może być zastosowanie Worker'ów, a~w nich WebAssembly. Dodatkowo można zastosować podejście SIMD. Niezbędna jest również analiza implementacji algorytmu CHT, który osiągnął niewspółmiernie długie czasy wykonania podczas testów metody opartej na WebGL. Na podstawie ogólnego rozeznania w~możliwościach każdej z~metod akceleracji,implementacji algorytmów oraz sposobów budowania bibliotek, możliwym staje się wybór interesujących elementów składowych algorytmów, ich ekstrakcja i~izolowane testy syntetyczne, co może również być przedmiotem dalszych badań. Możliwym kierunkiem dalszych badań z~pewnością powinna stać się implementacja i~analiza wydajności tych algorytmów dla akceleracji obliczeń z~użyciem WebGPU i~sprawdzenie, czy i~w jakim stopniu rozwiązuje on problemy wynikające z~niedoskonałości metody opartej na WebGL.

JavaScript, pomimo bycia rozwiniętym językiem z~bogatym ekosystemem, nie pozwala na budowę szeroko kompatybilnych bibliotek ze względu na różnice wśród środowisk i~ich różną specyfikę. Do najszybszych obliczeń, co pokazują przeprowadzone testy, i~prezentacji ich wyników najlepszym środowiskiem jest przeglądarka Google Chrome. Jednak konieczność budowania, trudność w~prototypowaniu rozwiązań oraz bycie środowiskiem odizolowanym od systemu operacyjnego skutecznie powstrzymało rozwój jego zastosowania w~obszarze obliczeń numerycznych. Środowiska serwerowe leżą po drugiej stronie barykady. Mają możliwość bezpośredniej interakcji z~systemem operacyjnym. Pomiędzy nimi występują jednak braki w~kompatybilności, czego przykładem jest interfejs obsługi Worker'ów w~NodeJS i~Deno. Prezentacja wyników w~formie graficznej wymaga wygenerowanie ich i~wyświetlenie w~postaci strony internetowej lub zapisania plików, podczas gdy w~języku Python biblioteki są w~stanie wyświetlić wyniki w~osobnych oknach.

Różnice środowisk, ich wzajemne wady i~zalety, oraz oczywiście docelowe zastosowanie samego języka były przyczyną powolnej ewolucji ekosystemu bibliotek, który jest w~tej chwili głównym hamulcem w~rozwoju tej gałęzi obliczeń w~języku JavaScript. Bibliotek jest mało, są mało popularne, co przekłada się na tempo ich rozwoju i~podążania za wydajnością i~kompatybilnością ze środowiskami. Jednak jak to zawsze bywa, najtrafniejszą odpowiedzią na większość pytań z~początku tego rozdziału będzie: \textit{To zależy}. W~zależności od wymaganych opóźnień i~zakładanego obciążenia konsekwencje asynchroniczności oraz braku bezpośredniego wsparcia dla wątków mogą stanowić problem. W~zależności od wymaganego środowiska dla wdrożenia aplikacji oraz zakładanego formatu prezentacji wyników środowisko przeglądarki może okazać się odpowiednie. W~zależności od implementowanego algorytmu będzie możliwe zastosowanie wybranych metod akceleracji. Pewnym jest jednak, że bezkompromisowe utworzenie biblioteki bez zważania na stosowana metodę akceleracji jest niemożliwe. Dla całego spektrum rozważań na temat algorytmów i~języków implementujących obliczenia numeryczne, te o~przystosowaniu środowisk języka JavaScript stanowią jedynie ich niewielką część. Powinny być one bowiem skoncentrowane w~pierwszej kolejności na złożoności obliczeniowej samych algorytmów, co jest najlepszym sposobem poprawy ich wydajności.

\clearpage
