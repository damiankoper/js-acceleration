\chapter{Metodologia pomiarów}

Testowanie wydajności języka JavaScript, który głównie wykorzystywany jest w przeglądarkach internetowych, z racji na ich szeroką kompatybilność oraz mnogość środowisk jest szczególnie problematyczny \cite{v8-real-perf}. Pierwotnie testy wydajności wykonywane były za pomocą testów syntetycznych, mikrobenchmarków - krótkich fragmentów kodu, które miały określić wydajność pojedynczej lub małego podzbioru funkcjonalności języka, na przykład porównując wydajność zwykłych tablic \mbox{\lstinline{Array}} do tablic typowanych, której przykładem jest obiekt \lstinline{Int8Array}. Popularnym narzędziem do budowanie takich benchmarków była strona jsPerf \cite{jsperf}, która obecnie nie jest już utrzymywana.

Rosnąca liczba API i elementów ekosystemu wykorzystująca coraz bardziej złożone mechanizmy doprowadziła do ewolucji mikrobenchmarków do statycznych zestawów testów. Przykładami takowych są wspomniany wcześniej Ostrich \cite{ostrich}, który wykonuje różnego rodzaju algorytmy numeryczne takie jak algorytm Bauma-Welcha, czy szybką transformację Fouriera. Innymi przykładami są benchmarki JetStream 2 oraz Octane sprawdzające, oprócz ogólnych algorytmów takich jak algorytmy sortowania, elementy specyficzne dla ekosystemu JavaScript takie jak czas kompilacji kompilatora TypeScript, działanie WebAssembly, czy wyrażeń regularnych \cite{octane, jetstream}.

Jednak w przeglądarkach internetowych z perspektywy ich głównego przeznaczenia liczy się wygoda użytkowania. Czas poświęcony na wykonywanie samego kodu JavaScript, razem z pobocznymi procesami takimi jak Garbage Collector, kompilacja i optymalizacja stanowią ok 40\% całego nakładu obliczeń przeglądarki, która musi oprócz tego parsować i renderować DOM oraz reagować na zdarzenia. Popularnym narzędziem do pomiarów wydajności strony z perspektywy czasu ładowania i renderowania jest Google Lighthouse \cite{lighthouse}. Popularnymi metrykami używanymi w tego typu pomiarach jest First Contentful paint, czyli czas do narysowania czegokolwiek na ekranie, czy Largest Contentful Paint, czyli czas po którym nastąpiła największe przerysowanie elementów strony. W środowiskach serwerowych problem ładowania strony naturalnie nie występuje. Mierzy się natomiast czas zimnego startu, czyli czasu wykonania kodu razem z czasem uruchomienia samego środowiska. Ma to szczególne znaczenie w środowiskach \textit{serverless} takich jak AWS Lambda \cite{aws-lambda}.

Prowadząc badania w ramach tej pracy nie musimy skupiać się na metrykach czasu ładowania strony lub uruchamiania środowiska serwerowego, ponieważ przy intensywnych lub powtarzalnych obliczeniach stanowią one pomijalną składową stałą.
Problem zimnego startu jednak występuje, choć w różnym stopniu. Wpływ na to mają procesy optymalizacji wykonania sekwencyjnego oraz czas inicjalizacji metod akceleracji \cite{je-benchmarking}.

Do pomiaru samego czasu wykonania można podejść na kilka sposobów. Dla krótkich fragmentów kodu istnieje ryzyko, że czas ich wykonania nie zmieści się w precyzji pomiaru czasu. Rozwiązaniem tego problemu jest pomiar czasu wykonania $t$ danej liczby iteracji $n$, a finalnym wynikiem pomiary będzie iloraz $\frac{t}{n}$. Innym podejściem jest wykonywanie testów w pętli, aż do osiągnięcia zadeklarowanego całkowitego czasu. Hybrydowym rozwiązaniem jest dynamiczne dostosowanie liczby cykli pojedynczego pomiaru czasu, aby zaspokoić wymagania całkowitego czasu testu.

Funkcje i pętle stosowane do sterowania cyklami, analizując krótkie czasy wykonania, mogą stanowić istotną składową stałą wyników. Aby temu zapobiec stosuje się dynamicznie tworzoną funkcję testową na podstawie serializowanej funkcji testowanej, co możliwe jest poprzez wywołania \lstinline{Function.prototype.toString()}. Wywołanie to zwraca funkcję - argumenty i jej ciało w postaci ciągu znaków, a następnie ciało duplikowane jest wymaganą liczbę razy. Rozwiązanie to, w przypadku bibliotek, które w procesie budowania przeszły proces minifikacji może prowadzić do błędów związanych z nazwami symboli. Dzieje się tak ponieważ w procesie minifikacji nazwy symboli, w celu zmniejszenia rozmiaru kodu, zamieniane są na ich krótsze odpowiedniki, często kolejne kombinacje liter alfabetu. Nie jest to możliwe dynamicznie w przypadku funkcji zserializowanej do ciągu znaków.

Problemem wartym rozważenie są również mechanizmy pomiaru czasu. Różnią się one w zależności od środowiska i w celu zapewnienia bezpieczeństwa mają one często ograniczoną rozdzielczość, aby zapobiec atakom czasowym jak Spectre i Meltdown. Standardowym rozwiązaniem kompatybilnym ze wszystkimi analizowanymi środowiskami jest Performance API z metodą \lstinline{performance.now()}, która zwraca liczbę zmiennoprzecinkową reprezentującą czas w milisekundach od załadowania strony. W przeglądarce Firefox wartość ta zaokrąglona jest to 1ms, a w Google Chrome do 0.1ms. Chrome po użyciu flagi \lstinline{--enable-benchmarking} przy uruchomieniu udostępnia obiekt \lstinline{chrome.Interval}, którego pomiary mają rozdzielczość $1 {\mu}s$. Deno po użyciu flagi \lstinline{--allow-hrtime} zwiększa rozdzielczość pomiarów przy użyciu Performance API, a NodeJS udostępnia obiekt \lstinline{process.hrtime}, którego pomiary wykonywane są z rozdzielczością $1ns$.

Biblioteka benchmark.js łączy przedstawione wcześniej rozwiązania, dynamicznie wykrywa środowisko, dynamicznie dostosowuje liczbę iteracji pojedynczego pomiaru czasu zgodnie ze zdefiniowanym czasem minimalnym i maksymalnym całego benchmarku oraz dynamicznie buduje funkcję testującą, która odpakowuje ciało funkcji testowanej, aby uniknąć narzutu związanego ze utworzeniem dodatkowej ramki na stosie, oraz aby móc wykorzystać zdefiniowane zmienne lokalne. Biblioteka ta jednak od 4 lat nie jest utrzymywana. Jako format modułu wykorzystuje format UMD, gdzie założeniem badań jest użycie wyłącznie modułów ECMAScript. Nie ma również możliwości zrezygnowania z dynamicznego budowania funkcji testującej, co prowadzi do błędów związanych z budowaniem środowiska testowego. Z tych powodów zadecydowano o własnej implementacji biblioteki do przeprowadzania benchmarków opartą o moduły ECMAScript oraz kompatybilną z badanymi środowiskami.

\section{Biblioteka benchmark}

% TODO plant diagram
